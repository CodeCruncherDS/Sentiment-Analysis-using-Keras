{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Doc2Vec?\n",
    "\n",
    "Doc2Vec is the straightforward extension of the word2vec model taking into account the overall vectorized form of the paragraph or the document. Word2vec creates vectors of only the words but this has a disadvantage where the model looses the overall meaning of the words. This is where Doc2Vec is important.\n",
    "It is based on the 2 word2vec architectures - skipgram(SG) and continuous bag of words(CBOW). The only difference is that apart from the word vectors, we also feed a paragraph-id as input to the neural network. Given below are the architectures defined in the original paper:\n",
    "\n",
    "**1. The first architecture is the Distributed Memory Model of Paragraph Vectors (PV-DM). This is similar to CBOW architecture of Word2Vec where, given a context we need to predict the word which would follow the sequence. However, we take a paragraph-token as a word and feed it into the neural net.**\n",
    "\n",
    "<img src=\"1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "**2. In the second architecture the only the paragraph token is fed as input to the neural network which then learns/predicts the words in a fixed context/window. This architecture is similar to the Skip-Gram model in Word2Vec and is known as Distributed Bag of Words version of Paragraph Vector (PV-DBOW)**\n",
    "\n",
    "<img src=\"2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "#Import necessary nlp tools from gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence, TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the text data\n",
    "\n",
    "We will use Cornell's IMDB Movie Review dataset for our sentiment analysis. We have four text files:\n",
    "- `test-neg.txt`: 12500 negative movie reviews from the test data\n",
    "- `test-pos.txt`: 12500 positive movie reviews from the test data\n",
    "- `train-neg.txt`: 12500 negative movie reviews from the training data\n",
    "- `train-pos.txt`: 12500 positive movie reviews from the training data\n",
    "\n",
    "I have already downloaded the preprocessed datasets but for someone collecting the dataset, one needs to carry out the required preprocessing such as removing punctuations and converting everything to lowercase.\n",
    "\n",
    "Each of the reviews should be formatted as such:\n",
    "\n",
    "```\n",
    "once again mr costner has dragged out a movie for far longer than necessary aside from the terrific sea rescue sequences of which there are very few i just did not care about any of the characters most of us have ghosts in the closet and costner s character are realized early on and then forgotten until much later by which time i did not care the character we should really care about is a very cocky overconfident ashton kutcher the problem is he comes off as kid who thinks he s better than anyone else around him and shows no signs of a cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tells us all about kutcher s ghosts we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it was all i could do to keep from turning it off an hour in\n",
    "this is an example of why the majority of action films are the same generic and boring there s really nothing worth watching here a complete waste of the then barely tapped talents of ice t and ice cube who ve each proven many times over that they are capable of acting and acting well don t bother with this one go see new jack city ricochet or watch new york undercover for ice t or boyz n the hood higher learning or friday for ice cube and see the real deal ice t s horribly cliched dialogue alone makes this film grate at the teeth and i m still wondering what the heck bill paxton was doing in this film and why the heck does he always play the exact same character from aliens onward every film i ve seen with bill paxton has him playing the exact same irritating character and at least in aliens his character died which made it somewhat gratifying overall this is second rate action trash there are countless better films to see and if you really want to see this one watch judgement night which is practically a carbon copy but has better acting and a better script the only thing that made this at all worth watching was a decent hand on the camera the cinematography was almost refreshing which comes close to making up for the horrible film itself but not quite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define source files for input data\n",
    "source_dict = {'test-neg.txt':'TEST_NEG',\n",
    "                'test-pos.txt':'TEST_POS',\n",
    "                'train-neg.txt':'TRAIN_NEG',\n",
    "                'train-pos.txt':'TRAIN_POS'\n",
    "               }\n",
    "\n",
    "\n",
    "\n",
    "# Define a LabeledDocSentence class to process multiple documents. This is an extension of the gensim's \n",
    "# LabeledLine class. Gensim's LabeledLine class does not process multiple documents, hence we need to define our own\n",
    "# implementation.\n",
    "class LabeledDocSentence():\n",
    "    \n",
    "    # Initialize the source dict\n",
    "    def __init__(self, source_dict):\n",
    "        self.sources = source_dict\n",
    "    \n",
    "    # This creates sentences as a list of words and assigns each sentence a tag \n",
    "    # e.g. [['word1', 'word2', 'word3', 'lastword'], ['label1']]\n",
    "    def create_sentences(self):\n",
    "        self.sentences = []\n",
    "        for source_file, prefix in self.sources.items():\n",
    "            with utils.smart_open(source_file) as f:\n",
    "                for line_id, line in enumerate(f):\n",
    "                    sentence_label = prefix + '_' + str(line_id)\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [sentence_label]))\n",
    "        \n",
    "        return self.sentences\n",
    "             \n",
    "    # Return a permutation of the sentences in each epoch. I read that this leads to the best results and \n",
    "    # helps the model to train properly.\n",
    "    def get_permuted_sentences(self):\n",
    "        sentences = self.create_sentences()\n",
    "        shuffled = list(sentences)\n",
    "        shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we use Gensim's Doc2Vec function to train our model on the sentences. There are various hyperparameters used in the function. Some of them are:\n",
    "- `min_count`: ignore all words with total frequency lower than this. You have to set this to 1, since the sentence labels only appear once. Setting it any higher than 1 will miss out on the sentences.\n",
    "- `window`: the maximum distance between the current and predicted word within a sentence. Word2Vec uses a skip-gram model, and this is simply the window size of the skip-gram model.\n",
    "- `size`: dimensionality of the feature vectors in output. 100 is a good number. If you're extreme, you can go up to around 400.\n",
    "- `sample`: threshold for configuring which higher-frequency words are randomly downsampled\n",
    "- `workers`: use this many worker threads to train the model \n",
    "\n",
    "I train the model for 10 epochs. It takes around 10 mins. We can use higher epochs for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_doc = LabeledDocSentence(source_dict) \n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "# Let the model learn the vocabulary - all the words in the paragraph\n",
    "model.build_vocab(labeled_doc.get_permuted_sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model on the entire set of sentences/reviews for 10 epochs. At each epoch sample a different\n",
    "# permutation of the sentences to make the model learn better.\n",
    "for epoch in range(10):\n",
    "    print epoch\n",
    "    model.train(labeled_doc.get_permuted_sentences(), total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To avoid retraining, we save the model\n",
    "model.save('imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_saved = Doc2Vec.load('imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'great', 0.7556788921356201),\n",
       " (u'decent', 0.7458477020263672),\n",
       " (u'nice', 0.7088463306427002),\n",
       " (u'bad', 0.6982812285423279),\n",
       " (u'solid', 0.6951433420181274),\n",
       " (u'fine', 0.6745833158493042),\n",
       " (u'excellent', 0.6504001021385193),\n",
       " (u'terrific', 0.629471480846405),\n",
       " (u'fantastic', 0.6224625110626221),\n",
       " (u'cool', 0.5988503098487854)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the model learned. It will show 10 most similar words to the input word. Since we kept the window size\n",
    "# to be 10, it will show the 10 most recent.\n",
    "model_saved.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.83013731,  1.28567719, -1.59022784,  1.13399196,  0.24096866,\n",
       "       -0.75068295,  1.56633914, -0.46217346, -2.37130737,  1.59989583,\n",
       "       -0.09220402, -0.24014057, -0.23156425, -1.29025269, -1.69817221,\n",
       "        1.69940197, -0.808604  ,  0.64784503, -0.69525599, -2.82596254,\n",
       "       -1.76106191,  2.25225616,  0.0821627 ,  2.27012062,  1.47031569,\n",
       "        1.83323991,  1.43627822, -0.91032636,  2.7978332 , -1.95226407,\n",
       "        0.16659185,  1.82336974, -0.36955601,  0.17694767, -0.3247571 ,\n",
       "        1.28920376, -0.15226717,  2.2388792 ,  0.02890237,  1.87150168,\n",
       "       -0.60525328,  0.12953719, -0.33312163, -0.97939563, -0.12537138,\n",
       "        0.5496937 ,  1.26948678,  0.55162621, -1.57944381,  0.18557446,\n",
       "        0.12860912, -1.0455178 ,  1.51583648,  1.01854277, -0.63605636,\n",
       "       -2.5904026 , -1.82550168,  0.69058442, -0.71976918,  0.97268033,\n",
       "       -0.58825153,  0.93084186, -2.34840751, -1.81852698,  0.10071582,\n",
       "       -0.47328663, -1.20318794, -1.39567649, -2.14003181, -0.59360862,\n",
       "       -0.75537062,  1.15100479,  0.35506916,  1.97487509, -2.1771822 ,\n",
       "       -0.36387426, -1.67836869, -1.21057034,  0.53206193,  2.77201104,\n",
       "        0.71309477,  0.02084459, -1.90477514,  1.54121232,  1.7257365 ,\n",
       "       -1.52941012,  0.0557668 ,  1.98820496,  0.76316643,  1.28354394,\n",
       "        0.7303769 , -1.20238495, -1.20528066, -0.63980323,  1.47038376,\n",
       "        0.76974547,  0.49791571, -0.25981614, -1.65224838,  1.34194458], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our model is a Doc2Vec model, hence it also learnt the sentence vectors apart from the word embeddings. Hence we\n",
    "# can see the vector of any sentence by passing the tag for the sentence.\n",
    "model_saved.docvecs['TRAIN_NEG_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labelled training and testing set\n",
    "\n",
    "x_train = np.zeros((25000, 100))\n",
    "y_train = np.zeros(25000)\n",
    "x_test = np.zeros((25000, 100))\n",
    "y_test = np.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    x_train[i] = model_saved.docvecs[prefix_train_pos]\n",
    "    x_train[12500 + i] = model_saved.docvecs[prefix_train_neg]\n",
    "    \n",
    "    y_train[i] = 1\n",
    "    y_train[12500 + i] = 0\n",
    "    \n",
    "    \n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_test_neg = 'TRAIN_NEG_' + str(i)\n",
    "    x_test[i] = model_saved.docvecs[prefix_test_pos]\n",
    "    x_test[12500 + i] = model_saved.docvecs[prefix_test_neg]\n",
    "    \n",
    "    y_test[i] = 1\n",
    "    y_test[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25382927 -0.71361071  0.98171765 ...,  0.06509715 -2.94681573\n",
      "  -2.84318161]\n",
      " [-0.10604549 -2.1016674   1.83482242 ..., -1.58855641 -2.93337131\n",
      "  -2.401752  ]\n",
      " [ 0.77429169  1.67526484 -1.24800324 ...,  0.34189934 -3.20372248\n",
      "   2.06218839]\n",
      " ..., \n",
      " [ 0.38597104 -2.45056629 -0.57511675 ..., -0.04185961 -1.80855238\n",
      "   0.34784812]\n",
      " [-0.37830129  0.77813071 -0.86706328 ..., -0.97749794 -0.0616008\n",
      "   0.31928027]\n",
      " [ 2.320539    0.19292563 -1.98491764 ..., -0.30593163  0.04952879\n",
      "   1.83004558]]\n"
     ]
    }
   ],
   "source": [
    "print x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the output to a categorical variable to be used for the 2 neuron output layer in the neural network.\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network with a single hidden layer and a softmax output layer with 2 neurons.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "nnet = Sequential()\n",
    "nnet.add(Dense(32, input_dim=100, activation='relu'))\n",
    "nnet.add(Dense(2, input_dim=32, activation='softmax'))\n",
    "nnet.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 3,298\n",
      "Trainable params: 3,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Visualize the neural net's layer\n",
    "nnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 2s - loss: 0.3940 - acc: 0.8311     \n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1s - loss: 0.3253 - acc: 0.8642     \n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 1s - loss: 0.3107 - acc: 0.8682     \n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 1s - loss: 0.3008 - acc: 0.8740     \n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 1s - loss: 0.2928 - acc: 0.8781     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a2f9e50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the net on the training data\n",
    "nnet.fit(x_train, y_train_cat, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22784/25000 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88331999999999999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "score = nnet.evaluate(x_test, y_test_cat, batch_size=32)\n",
    "score[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
